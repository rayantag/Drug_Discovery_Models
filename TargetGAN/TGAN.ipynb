{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installations + Imports"
      ],
      "metadata": {
        "id": "m6Vhz7PgAhrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rdkit"
      ],
      "metadata": {
        "id": "zhzZ-XQTlQxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tape_proteins"
      ],
      "metadata": {
        "id": "QdbdOL67lZJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "BJAFU8hilgni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install deepchem"
      ],
      "metadata": {
        "id": "EqcgoaBuuXpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "from tape import ProteinBertModel, TAPETokenizer\n",
        "import torch.nn as nn\n",
        "from rdkit import Chem\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoModel, AutoTokenizer, GPTNeoForCausalLM"
      ],
      "metadata": {
        "id": "z3oM3eXGBXFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Dataset + Create DataLoaders"
      ],
      "metadata": {
        "id": "8SeLIDT6Alk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess():\n",
        "  df = pd.read_csv('SMILES_Keyboard.csv')\n",
        "  smiles_list = df['SMILES'].tolist()\n",
        "  # Truncate the length of the SMILES strings to conserve memory and power.\n",
        "  smiles_list = [word for word in smiles_list if len(word) <= 75]\n",
        "\n",
        "  unique_chars = set()\n",
        "\n",
        "  for smiles in smiles_list:\n",
        "      for char in smiles:\n",
        "          unique_chars.add(char)\n",
        "\n",
        "  vocab = sorted(list(unique_chars))\n",
        "\n",
        "  # Add special tokens\n",
        "  START_TOKEN = '$'\n",
        "  END_TOKEN = '&'\n",
        "  PAD_TOKEN = '^'\n",
        "\n",
        "  vocab.extend([START_TOKEN, END_TOKEN, PAD_TOKEN])\n",
        "\n",
        "  char_to_index = {char: idx for idx, char in enumerate(vocab)}\n",
        "  index_to_char = {idx: char for char, idx in char_to_index.items()}\n",
        "\n",
        "  # Define a maximum length for the sequences\n",
        "  # It should be at least one more than the length of the longest SMILES string\n",
        "  # in your dataset to account for the start and end tokens.\n",
        "  MAX_LENGTH = 77\n",
        "\n",
        "  def one_hot_encode(smiles, vocab):\n",
        "      # Create an array of zeros with the shape (MAX_LENGTH, length of the vocabulary)\n",
        "      encoded = np.zeros((MAX_LENGTH, len(vocab)), dtype=int)\n",
        "\n",
        "      # Add start token\n",
        "      smiles = START_TOKEN + smiles + END_TOKEN\n",
        "      if len(smiles) < MAX_LENGTH:\n",
        "        padding_length = MAX_LENGTH - len(smiles)\n",
        "        smiles = smiles + PAD_TOKEN * padding_length\n",
        "      # Go through each character in the SMILES string\n",
        "      for i, char in enumerate(smiles):\n",
        "          # Find the position of the character in the vocabulary\n",
        "          j = vocab.index(char)\n",
        "          # Set the corresponding position in the encoded matrix to 1\n",
        "          encoded[i, j] = 1\n",
        "\n",
        "      # The rest of the positions in the matrix will remain 0, representing the PAD token\n",
        "\n",
        "      return encoded\n",
        "\n",
        "  encoded_data = [one_hot_encode(smiles, vocab) for smiles in smiles_list]\n",
        "\n",
        "  train, val = train_test_split(encoded_data, test_size=0.2, random_state=69)"
      ],
      "metadata": {
        "id": "LmaAyLl8Ac8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SMILESDataset(Dataset):\n",
        "    def __init__(self, word_list):\n",
        "        self.word_list = word_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.word_list[idx]"
      ],
      "metadata": {
        "id": "6-z_5ktbAfqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = SMILESDataset(train)\n",
        "val_dataset = SMILESDataset(val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "kKzjz60OAsou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Architecture. 3 modules: AASE, SFI, MG"
      ],
      "metadata": {
        "id": "z_OV6cAUA4b8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Amino Acid Sequence Embedding module.\n",
        "\n",
        "class AASEModule:\n",
        "    def __init__(self):\n",
        "        self.model = ProteinBertModel.from_pretrained('bert-base')\n",
        "        self.tokenizer = TAPETokenizer(vocab='iupac')\n",
        "\n",
        "    def get_embedding(self, sequence):\n",
        "        token_ids = torch.tensor([self.tokenizer.encode(sequence)])\n",
        "        with torch.no_grad():  # No need to compute gradients for this step\n",
        "            sequence_output, pooled_output = self.model(token_ids)\n",
        "\n",
        "        # Since the pooled output is not trained, take the mean of the sequence output\n",
        "        sequence_mean = sequence_output.mean(dim=1)\n",
        "\n",
        "        return sequence_mean"
      ],
      "metadata": {
        "id": "pu0_Lbp0BoOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perceptron layer for Structural Feature Inference.\n",
        "\n",
        "class Perceptron(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim=512):\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "protein_feature_dim = 768\n",
        "perceptron = Perceptron(protein_feature_dim)"
      ],
      "metadata": {
        "id": "trlwhSHQBy5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GANs to learn how to convert AASE Embedding into protein latent representation.\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim, condition_dim, output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(noise_dim+condition_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, condition):\n",
        "        x = torch.cat([noise, condition], dim=1)\n",
        "        return self.fc(x)\n",
        "\n",
        "noise_dim = 100\n",
        "generator_output_dim = 512\n",
        "generator = Generator(noise_dim, 868, generator_output_dim)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "discriminator = Discriminator(generator_output_dim)"
      ],
      "metadata": {
        "id": "WOktJnXwB1Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Molecular Generation conditioned on latent representation.\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size=42, seq_len=77, hidden_dim=512, latent_dim=512):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.encoder_lstm = nn.LSTM(vocab_size, hidden_dim//2, num_layers=2, bidirectional=True, batch_first=True)\n",
        "        self.encoder_ff = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, (h_n, c_n) = self.encoder_lstm(x)\n",
        "        h_final_forward = h_n[-2]\n",
        "        h_final_backward = h_n[-1]\n",
        "        h_final = torch.cat([h_final_forward, h_final_backward], dim=1)\n",
        "        latent = self.encoder_ff(h_final)\n",
        "\n",
        "        return latent\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size=42, hidden_dim=512):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.decoder_lstm = nn.LSTM(vocab_size, hidden_dim, num_layers=4, batch_first=True)\n",
        "        self.decoder_ff = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, latent):\n",
        "        lstm_out, _ = self.decoder_lstm(x, (latent.unsqueeze(0).repeat(4, 1, 1), torch.zeros_like(latent).unsqueeze(0).repeat(4, 1, 1)))\n",
        "        output = self.decoder_ff(lstm_out)\n",
        "\n",
        "        return output\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, vocab_size=42, seq_len=77, hidden_dim=512, latent_dim=512):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(vocab_size, seq_len, hidden_dim, latent_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        output = self.decoder(x, latent)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "X5um08JFV6Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training + Inference"
      ],
      "metadata": {
        "id": "xAEhbBv9BMvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "model = Autoencoder()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 4\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(tqdm(train_loader)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch = batch.float()\n",
        "        outputs = model(batch)\n",
        "\n",
        "        labels = torch.argmax(batch, dim=-1)\n",
        "        loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # Validation.\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = batch.float()\n",
        "            outputs = model(batch)\n",
        "            labels = torch.argmax(batch, dim=-1)\n",
        "            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "torch.save(model, 'TGan_weights.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t315-FtoVVcI",
        "outputId": "22405af8-3bfc-4818-c8a7-e5dab3a04cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 442/442 [19:39<00:00,  2.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4, Training Loss: 0.8146875046058748, Validation Loss: 0.11592584096633636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 442/442 [20:14<00:00,  2.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/4, Training Loss: 0.05951944710813227, Validation Loss: 0.03122221213673149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 442/442 [20:52<00:00,  2.83s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/4, Training Loss: 0.1680420453584336, Validation Loss: 0.06258153244181797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 442/442 [21:32<00:00,  2.92s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/4, Training Loss: 0.037058004810226176, Validation Loss: 0.022935522292312736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DrugGeneratorModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DrugGeneratorModel, self).__init__()\n",
        "\n",
        "        # 1. AASEModule for amino acid sequence embeddings\n",
        "        self.aase_module = AASEModule()\n",
        "\n",
        "        # 2. Generator and Discriminator\n",
        "        self.generator = Generator(noise_dim, 768, generator_output_dim)\n",
        "        self.discriminator = Discriminator(generator_output_dim)\n",
        "\n",
        "    def amino_to_embedding(self, amino_sequence):\n",
        "        return self.aase_module.get_embedding(amino_sequence)\n",
        "\n",
        "    def generate(self, amino_embedding, noise):\n",
        "        return self.generator(noise, amino_embedding)\n",
        "\n",
        "    def discriminate(self, samples):\n",
        "        return self.discriminator(samples)\n"
      ],
      "metadata": {
        "id": "wNYgj3DmVLem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the combined model\n",
        "dGEN = DrugGeneratorModel()\n",
        "\n",
        "# 1. Convert amino acid sequence to embedding.\n",
        "sequence = 'GCTVEDRCLIGMGAILLNGCVIGSGSLVAAGALITQ'\n",
        "amino_embedding = dGEN.amino_to_embedding(sequence)\n",
        "\n",
        "# 2. Generate some random noise.\n",
        "noise = torch.randn((1, noise_dim))\n",
        "\n",
        "# 3. Get the generated output\n",
        "generated_latent = dGEN.generate(amino_embedding, noise)\n",
        "\n",
        "# Convert PyTorch tensor to numpy for TensorFlow model.\n",
        "generated_latent_np = generated_latent.detach().numpy()\n",
        "print(generated_latent_np.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3bRylVLVXLU",
        "outputId": "794b68b0-91a0-4ca4-e437-c546d8447e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 567/567 [00:00<00:00, 1626655.52B/s]\n",
            "100%|██████████| 370264230/370264230 [00:10<00:00, 36771088.48B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-9f77f90674b1>:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  token_ids = torch.tensor([self.tokenizer.encode(sequence)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_smiles(decoder, latent_vector, start_token, char_to_index, index_to_char, max_length=77):\n",
        "\n",
        "    # Convert numpy ndarray to PyTorch tensor if necessary.\n",
        "    if isinstance(latent_vector, np.ndarray):\n",
        "        latent_vector = torch.tensor(latent_vector).float()\n",
        "\n",
        "    # Ensure the tensor is in the expected shape: (1, latent_dim).\n",
        "    if len(latent_vector.shape) == 1:\n",
        "        latent_vector = latent_vector.unsqueeze(0)\n",
        "\n",
        "    # Initialize the sequence with the START token.\n",
        "    input_sequence = torch.zeros(1, 1, len(char_to_index)).float()\n",
        "    input_sequence[0, 0, char_to_index[start_token]] = 1\n",
        "\n",
        "    # The generated sequence starts with the START token.\n",
        "    generated_sequence = [start_token]\n",
        "\n",
        "    # Begin iterative generation\n",
        "    for _ in range(max_length):\n",
        "        # Feed the input sequence and the latent vector into the decoder.\n",
        "        output = decoder(input_sequence, latent_vector)\n",
        "\n",
        "        # Get the character with the highest prediction probability.\n",
        "        _, predicted_idx = output.topk(1)\n",
        "        predicted_idx = predicted_idx[0, -1, 0].item()\n",
        "        predicted_char = index_to_char[predicted_idx]\n",
        "\n",
        "        # Append the predicted character to the sequence.\n",
        "        generated_sequence.append(predicted_char)\n",
        "\n",
        "        # Stop generation if END token is produced.\n",
        "        if predicted_char == END_TOKEN:\n",
        "            break\n",
        "\n",
        "        # Update the input_sequence for the next iteration.\n",
        "        input_sequence = torch.zeros(1, 1, len(char_to_index)).float()\n",
        "        input_sequence[0, 0, predicted_idx] = 1\n",
        "\n",
        "    return ''.join(generated_sequence)\n"
      ],
      "metadata": {
        "id": "F9AaKXxlHw5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smiles_to_tensor(smiles, char_to_index, max_length=77):\n",
        "    tensor = torch.zeros(max_length, len(char_to_index)).float()\n",
        "    for i, char in enumerate(smiles):\n",
        "        tensor[i, char_to_index[char]] = 1\n",
        "    return tensor\n",
        "\n",
        "def tensor_to_smiles(tensor, index_to_char):\n",
        "    smiles = []\n",
        "    _, indices = tensor.topk(1, dim=-1)\n",
        "    for idx in indices.squeeze():\n",
        "        char = index_to_char[idx.item()]\n",
        "        if char == END_TOKEN:\n",
        "            break\n",
        "        smiles.append(char)\n",
        "    return ''.join(smiles)\n",
        "\n",
        "def encode_decode_smiles(encoder, decoder, smiles, char_to_index, index_to_char, max_length=77):\n",
        "    # Convert smiles to tensor.\n",
        "    input_tensor = smiles_to_tensor(smiles, char_to_index, max_length)\n",
        "    input_tensor = input_tensor.unsqueeze(0)\n",
        "\n",
        "    # Pass through the encoder.\n",
        "    latent_vector = encoder(input_tensor)\n",
        "\n",
        "    # Pass the latent vector through the decoder.\n",
        "    output_tensor = decoder(input_tensor, latent_vector)\n",
        "\n",
        "    # Convert tensor back to SMILES.\n",
        "    reconstructed_smiles = tensor_to_smiles(output_tensor.squeeze(), index_to_char)\n",
        "\n",
        "    return reconstructed_smiles\n",
        "\n",
        "# Usage:\n",
        "smiles_string = \"$CN1CC(C2=NCCN2)Oc2c1ccc1ccccc21&^^^^^^^^^^^^^^^^^\"\n",
        "reconstructed = encode_decode_smiles(model.encoder, model.decoder, smiles_string, char_to_index, index_to_char)\n",
        "print(reconstructed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLKTxGtNUVuP",
        "outputId": "e5fbeae3-cf7b-4a28-b3f4-dd2f54ed7c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$CN1CC(C2=NCCN2)Oc2c1ccc1ccccc21\n"
          ]
        }
      ]
    }
  ]
}